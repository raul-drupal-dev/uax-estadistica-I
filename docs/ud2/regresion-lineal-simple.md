title: "RegresiÃ³n lineal simple"
slug: "ud2-regresion-lineal-simple"
date: "2026-01-24"
authors: ["Profesor UAX"]
tags: ["ud2", "regresion", "minimos-cuadrados", "prediccion", "bivariante"]
difficulty: "intermedio"
type: "definicion"
prerequisitos: ["ud2-correlacion-covarianza"]

---

## Objetivo

âœ¨ Entender cÃ³mo **construir un modelo lineal para predecir Y a partir de X**, estimando la recta de regresiÃ³n y evaluando su ajuste.

## Idea Clave ğŸ’¡

En lugar de solo medir correlaciÃ³n, vamos a **encontrar la mejor recta que se ajusta a los datos**, permitiÃ©ndonos predecir nuevos valores de Y dados X. Esta recta minimiza el error de predicciÃ³n.

---

## El Modelo de RegresiÃ³n Lineal Simple

### DefiniciÃ³n

Un **modelo de regresiÃ³n lineal simple** es una relaciÃ³n lineal entre una variable predictora (X, independiente) y una variable respuesta (Y, dependiente):

$$Y = \beta_0 + \beta_1 X + \epsilon$$

**Componentes:**

- **Î²â‚€** (intercept): la ordenada al origen (donde cruza el eje Y cuando X=0)
- **Î²â‚** (pendiente): cÃ³mo cambia Y por cada unidad de X
- **Îµ** (error): la variabilidad no explicada por la recta

### EstimaciÃ³n: MÃ©todo de MÃ­nimos Cuadrados

Nuestro objetivo es encontrar $\hat{\beta}_0$ y $\hat{\beta}_1$ que **minimicen la suma de cuadrados de los residuos**:

$$\text{Minimizar} \quad \sum_{i=1}^n (y_i - \hat{y}_i)^2$$

donde $\hat{y}_i = \hat{\beta}_0 + \hat{\beta}_1 x_i$ es el valor predicho.

**Las fÃ³rmulas resultantes son:**

$$\hat{\beta}_1 = \frac{\text{Cov}(X, Y)}{\text{Var}(X)} = \frac{\sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^n (x_i - \bar{x})^2}$$

$$\hat{\beta}_0 = \bar{y} - \hat{\beta}_1 \bar{x}$$

**Nota:** $\hat{\beta}_1 = r_{XY} \frac{\sigma_Y}{\sigma_X}$ (relaciÃ³n con la correlaciÃ³n)

???+ example "Estimar la Recta de RegresiÃ³n"

    Datos:
    ```
    X (Horas estudiadas): [2, 4, 6, 8, 10]
    Y (CalificaciÃ³n 0-10): [3, 5, 6, 8, 9]
    ```

    **Paso 1:** Calcular medias y varianzas

    - $\bar{x} = 6$, $\bar{y} = 6.2$
    - $\sum (x_i - \bar{x})^2 = 4 + 4 + 0 + 4 + 16 = 28$
    - $$
      \begin{aligned}
      \sum (x_i - \bar{x})(y_i - \bar{y}) &= (-4)(-3.2) + (-2)(-1.2) + 0(-0.2) + 2(1.8) + 4(2.8) \\
      &= 12.8 + 2.4 + 0 + 3.6 + 11.2 \\
      &= 30
      \end{aligned}
      $$

    **Paso 2:** Calcular pendiente e intercept

    $$\hat{\beta}_1 = \frac{30}{28} = 1.071$$

    $$\hat{\beta}_0 = 6.2 - 1.071 \times 6 = 6.2 - 6.426 = -0.226 â‰ˆ -0.23$$

    **EcuaciÃ³n de la recta:**

    $$\hat{Y} = -0.23 + 1.071 X$$

    **PredicciÃ³n:** Si estudias 7 horas, tu calificaciÃ³n predicha serÃ­a:

    $$\hat{Y} = -0.23 + 1.071 \times 7 = 7.27 â‰ˆ 7.3$$

---

## InterpretaciÃ³n de ParÃ¡metros

### Pendiente Î²â‚

- **Î²â‚ = 1.071** significa: por cada hora adicional de estudio, la calificaciÃ³n esperada aumenta **1.071 puntos** (aproximadamente 1 punto por hora)

- **Î²â‚ > 0:** RelaciÃ³n positiva (X aumenta âŸ¹ Y aumenta)
- **Î²â‚ < 0:** RelaciÃ³n negativa (X aumenta âŸ¹ Y disminuye)
- **Î²â‚ = 0:** No hay relaciÃ³n lineal (modelo no es Ãºtil)

### Intercept Î²â‚€

- **Î²â‚€ = -0.23** significa: si estudias 0 horas, el modelo predice una calificaciÃ³n de -0.23 (lo cual no tiene sentido educativo, pues no puedes tener calificaciÃ³n negativa).

!!! warning "Cuidado con extrapolaciÃ³n"

    No uses el modelo para predecir valores de X fuera del rango de los datos. En nuestro ejemplo, el rango es [2, 10], asÃ­ que predecir para X=0 es extrapolaciÃ³n y no es confiable.

---

## Residuos y Bondad de Ajuste

### Residuo

El **residuo** para la observaciÃ³n i es:

$$e_i = y_i - \hat{y}_i$$

(valor real menos valor predicho)

**Propiedades:**

- Si e_i > 0: el modelo subestimÃ³ (predicciÃ³n fue menor que el valor real)
- Si e_i < 0: el modelo sobreestimÃ³ (predicciÃ³n fue mayor que el valor real)
- Los residuos deben tener media cercana a 0

### Coeficiente de DeterminaciÃ³n (RÂ²)

**RÂ²** es la fracciÃ³n de variabilidad en Y explicada por X:

$$R^2 = \frac{\text{Variabilidad explicada}}{\text{Variabilidad total}} = \frac{\sum_i (\hat{y}_i - \bar{y})^2}{\sum_i (y_i - \bar{y})^2}$$

TambiÃ©n: $R^2 = r_{XY}^2$ (el cuadrado de la correlaciÃ³n)

**InterpretaciÃ³n:**

- **RÂ² = 1:** Ajuste perfecto (recta pasa por todos los puntos)
- **RÂ² = 0.8:** El modelo explica 80% de la variabilidad en Y
- **RÂ² = 0.2:** Pobre ajuste (mucha variabilidad no explicada)
- **RÂ² = 0:** La recta no explica nada (modelo inÃºtil)

???+ example "Calcular RÂ² en Nuestro Ejemplo"

    Del ejemplo anterior, tenemos:
    
    - Valores predichos: $\hat{y} = [-0.23 + 1.071(2), -0.23 + 1.071(4), ...]  = [1.91, 4.05, 6.19, 8.34, 10.48]$

    (Nota: el Ãºltimo predice 10.48, pero el mÃ¡ximo observado es 9, asÃ­ que hay extrapolaciÃ³n)

    Suma de cuadrados totales:

    $$SST = (3-6.2)^2 + (5-6.2)^2 + ... = 10.24 + 1.44 + 0.04 + 3.24 + 8.41 = 23.37$$

    Suma de cuadrados residuales (diferencias entre observado y predicho):

    $$SSE = (3-1.91)^2 + (5-4.05)^2 + (6-6.19)^2 + ... = 1.19 + 0.90 + 0.04 + 0.11 + 0.23 = 2.47$$

    $$R^2 = \frac{SST - SSE}{SST} = \frac{23.37 - 2.47}{23.37} = \frac{20.90}{23.37} = 0.894$$

    **RÂ² = 0.894 â‰ˆ 0.89**, lo que significa que el modelo explica **89% de la variabilidad** en calificaciones. âœ…

---

## Supuestos del Modelo

La regresiÃ³n lineal asume:

1. **Linealidad:** La relaciÃ³n entre X e Y es lineal (no vale para relaciones curvas)
2. **Homocedasticidad:** Los errores tienen varianza constante (no aumenta con X)
3. **Normalidad:** Los errores estÃ¡n normalmente distribuidos
4. **Independencia:** Los errores de diferentes observaciones son independientes
5. **Sin multicolinealidad:** (En regresiÃ³n mÃºltiple, variables independientes no estÃ¡n correlacionadas)

!!! tip "DiagnÃ³stico"

    En la prÃ¡ctica, haz grÃ¡ficos de:
    - Scatter plot (X vs Y con recta superpuesta) para ver linealidad
    - Residuos vs valores predichos para ver homocedasticidad

---

## âš ï¸ Trampa ComÃºn: Usar RÂ² sin pensar

âŒ **Incorrecto:** "RÂ² = 0.6 es suficientemente bueno para todos los usos"

âœ… **Correcto:** El valor aceptable de RÂ² depende del contexto:

- En predicciÃ³n de fenÃ³menos fÃ­sicos exactos: RÂ² > 0.95 es normal
- En predicciÃ³n de comportamiento humano: RÂ² > 0.3 es razonable
- En ciencias sociales: RÂ² > 0.1 puede ser valioso

---

## Tabla Resumen: RegresiÃ³n Lineal Simple

| **Concepto**  | **FÃ³rmula**                                              | **InterpretaciÃ³n**                   |
| ------------- | -------------------------------------------------------- | ------------------------------------ |
| **Recta**     | $\hat{Y} = \hat{\beta}_0 + \hat{\beta}_1 X$              | PredicciÃ³n lineal                    |
| **Pendiente** | $\hat{\beta}_1 = \frac{\text{Cov}(X, Y)}{\text{Var}(X)}$ | Cambio en Y por unidad de X          |
| **Intercept** | $\hat{\beta}_0 = \bar{y} - \hat{\beta}_1 \bar{x}$        | Valor de Y cuando X=0                |
| **Residuo**   | $e_i = y_i - \hat{y}_i$                                  | Error de predicciÃ³n                  |
| **RÂ²**        | $r_{XY}^2$                                               | ProporciÃ³n de variabilidad explicada |

---

## ğŸ’¡ Checklist

!!! tip "Antes de datos categÃ³ricos"

    - [ ] Â¿Sabes calcular $\hat{\beta}_0$ y $\hat{\beta}_1$ manualmente?
    - [ ] Â¿Puedes interpretar la pendiente en contexto?
    - [ ] Â¿Entiendes quÃ© significa RÂ² = 0.75?
    - [ ] Â¿Sabes la diferencia entre correlaciÃ³n y causalidad en regresiÃ³n?
    - [ ] Â¿Puedes hacer una predicciÃ³n usando la recta estimada?

---

## ğŸ“– Enlaces Relacionados

- [CorrelaciÃ³n y covarianza](./correlacion-covarianza.md) â€” Base para regresiÃ³n
- [Bivariante â€” IntroducciÃ³n](./bivariante-intro.md) â€” Conceptos previos
- [Datos categÃ³ricos](./datos-categoricos.md) â€” Alternativas para variables categÃ³ricas
